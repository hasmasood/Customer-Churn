{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     active environment : aws_env\n",
      "    active env location : /home/hassan101/anaconda3/envs/aws_env\n"
     ]
    }
   ],
   "source": [
    "#Active environment should be aws_env\n",
    "!conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active User: usr_hassan\n"
     ]
    }
   ],
   "source": [
    "#Get AWS credentials from environment\n",
    "import os\n",
    "aws_akid = os.environ['AWS_KID']\n",
    "aws_sak = os.environ['AWS_AK']\n",
    "\n",
    "import boto3\n",
    "client = boto3.client('iam', aws_access_key_id=aws_akid, aws_secret_access_key= aws_sak)\n",
    "users = client.list_users()\n",
    "for key in users['Users']:\n",
    "    print('Active User:', key['UserName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting train and test splits locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4930, 20)\n",
      "(4930, 1)\n",
      "(2113, 20)\n",
      "(2113, 1)\n"
     ]
    }
   ],
   "source": [
    "Xy_train = pd.read_csv('Xy_train.csv', index_col=0)\n",
    "Xy_test = pd.read_csv('Xy_test.csv', index_col=0)\n",
    "\n",
    "X_train = Xy_train.iloc[:,:-1]\n",
    "X_test = Xy_test.iloc[:,:-1]\n",
    "\n",
    "y_train = Xy_train.iloc[:,[-1]]\n",
    "y_test = Xy_test.iloc[:,[-1]]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Col: ['tenure', 'monthlycharges', 'totalcharges']\n",
      "Cat Col: ['customerid', 'gender', 'seniorcitizen', 'partner', 'dependents', 'phoneservice', 'multiplelines', 'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling', 'paymentmethod']\n"
     ]
    }
   ],
   "source": [
    "#Get names of numerical and categorical columns\n",
    "from pandas.api.types import is_object_dtype, is_numeric_dtype, is_bool_dtype\n",
    "\n",
    "num_col = []\n",
    "cat_col = []\n",
    "\n",
    "#Scanning all feature columns in X_train\n",
    "for col in X_train.columns: \n",
    "    if is_object_dtype(X_train[col]):\n",
    "        cat_col.append(col)\n",
    "    else:\n",
    "        num_col.append(col)\n",
    "\n",
    "print('Num Col:',num_col)\n",
    "print('Cat Col:', cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'seniorcitizen',\n",
       " 'partner',\n",
       " 'dependents',\n",
       " 'phoneservice',\n",
       " 'multiplelines',\n",
       " 'internetservice',\n",
       " 'onlinesecurity',\n",
       " 'onlinebackup',\n",
       " 'deviceprotection',\n",
       " 'techsupport',\n",
       " 'streamingtv',\n",
       " 'streamingmovies',\n",
       " 'contract',\n",
       " 'paperlessbilling',\n",
       " 'paymentmethod']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing customer id from cat_col\n",
    "cat_col.remove('customerid')\n",
    "cat_col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use dic vectorizer for OHE\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "X_train_dicts = X_train[cat_col + num_col].to_dict(orient='records') #orient=records ensures records are based on rows not columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created a list of dictionary for X_train dataset, with each row of dataset as a dictioary in the list. This is how the first row in dictionary list looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 'male',\n",
       " 'seniorcitizen': 'no',\n",
       " 'partner': 'no',\n",
       " 'dependents': 'no',\n",
       " 'phoneservice': 'yes',\n",
       " 'multiplelines': 'yes',\n",
       " 'internetservice': 'dsl',\n",
       " 'onlinesecurity': 'no',\n",
       " 'onlinebackup': 'yes',\n",
       " 'deviceprotection': 'no',\n",
       " 'techsupport': 'yes',\n",
       " 'streamingtv': 'yes',\n",
       " 'streamingmovies': 'no',\n",
       " 'contract': 'one_year',\n",
       " 'paperlessbilling': 'yes',\n",
       " 'paymentmethod': 'electronic_check',\n",
       " 'tenure': 20,\n",
       " 'monthlycharges': 68.7,\n",
       " 'totalcharges': 1416.2}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dicts[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create dictionary vectorizer. The `sparse=True` argument will produce sparse matrix in sparse row format (special way of encoding data when there are many zeros - may work well when there are too many distinct values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 0.0000e+00, 6.8700e+01, 0.0000e+00,\n",
       "       0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 2.0000e+01,\n",
       "       1.4162e+03])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['contract=month-to-month', 'contract=one_year',\n",
       "       'contract=two_year', 'dependents=no', 'dependents=yes',\n",
       "       'deviceprotection=no', 'deviceprotection=no_internet_service',\n",
       "       'deviceprotection=yes', 'gender=female', 'gender=male',\n",
       "       'internetservice=dsl', 'internetservice=fiber_optic',\n",
       "       'internetservice=no', 'monthlycharges', 'multiplelines=no',\n",
       "       'multiplelines=no_phone_service', 'multiplelines=yes',\n",
       "       'onlinebackup=no', 'onlinebackup=no_internet_service',\n",
       "       'onlinebackup=yes', 'onlinesecurity=no',\n",
       "       'onlinesecurity=no_internet_service', 'onlinesecurity=yes',\n",
       "       'paperlessbilling=no', 'paperlessbilling=yes', 'partner=no',\n",
       "       'partner=yes', 'paymentmethod=bank_transfer_(automatic)',\n",
       "       'paymentmethod=credit_card_(automatic)',\n",
       "       'paymentmethod=electronic_check', 'paymentmethod=mailed_check',\n",
       "       'phoneservice=no', 'phoneservice=yes', 'seniorcitizen=no',\n",
       "       'seniorcitizen=yes', 'streamingmovies=no',\n",
       "       'streamingmovies=no_internet_service', 'streamingmovies=yes',\n",
       "       'streamingtv=no', 'streamingtv=no_internet_service',\n",
       "       'streamingtv=yes', 'techsupport=no',\n",
       "       'techsupport=no_internet_service', 'techsupport=yes', 'tenure',\n",
       "       'totalcharges'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4930, 46)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(X_train_dicts)\n",
    "Xohe_train_array = dv.transform(X_train_dicts)\n",
    "\n",
    "display(Xohe_train_array[0])\n",
    "display(dv.get_feature_names_out())\n",
    "display(Xohe_train_array.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2113, 46)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dicts = X_test[cat_col + num_col].to_dict(orient='records')\n",
    "Xohe_test_array = dv.transform(X_test_dicts)\n",
    "Xohe_test_array.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training via pipelienes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression', LogisticRegression())])\n",
      "rf\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier', RandomForestClassifier())])\n",
      "gb\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('gradientboostingclassifier', GradientBoostingClassifier())])\n"
     ]
    }
   ],
   "source": [
    "#Setup MLDev pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "mldev_pipeline = {\n",
    "    'logreg': make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier())\n",
    "}\n",
    "\n",
    "for model_name, pipeline in mldev_pipeline.items():\n",
    "    print(model_name)\n",
    "    print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These can also be viewed visually\n",
    "mldev_pipeline['logreg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinReg Params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "RF Params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "GB Params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#Check the parameters for these models\n",
    "print('LinReg Params:',LogisticRegression().get_params())\n",
    "print('RF Params:',RandomForestClassifier().get_params())\n",
    "print('GB Params:',GradientBoostingClassifier().get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a grid for hyperparameters tuning\n",
    "hpgrid = {\n",
    "\n",
    "    'logreg':{\n",
    "    },\n",
    "    \n",
    "    'rf':{\n",
    "        'randomforestclassifier__n_estimators':[100, 200, 300],\n",
    "        'randomforestclassifier__max_depth':[5, 10, 'None']\n",
    "    },\n",
    "\n",
    "    'gb':{\n",
    "        'gradientboostingclassifier__n_estimators':[100,200,300],\n",
    "        'gradientboostingclassifier__learning_rate':[0.1, 0.2],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: logreg\n",
      "Training model: rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 341, in fit\n",
      "    self._validate_params()\n",
      "  File \"/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/hassan101/anaconda3/envs/aws_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.79066937 0.79208925 0.79066937 0.79371197 0.79432049 0.79472617\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: gb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logreg': GridSearchCV(cv=10,\n",
       "              estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                        ('logisticregression',\n",
       "                                         LogisticRegression())]),\n",
       "              n_jobs=-1, param_grid={}),\n",
       " 'rf': GridSearchCV(cv=10,\n",
       "              estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                        ('randomforestclassifier',\n",
       "                                         RandomForestClassifier())]),\n",
       "              n_jobs=-1,\n",
       "              param_grid={'randomforestclassifier__max_depth': [5, 10, 'None'],\n",
       "                          'randomforestclassifier__n_estimators': [100, 200,\n",
       "                                                                   300]}),\n",
       " 'gb': GridSearchCV(cv=10,\n",
       "              estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                        ('gradientboostingclassifier',\n",
       "                                         GradientBoostingClassifier())]),\n",
       "              n_jobs=-1,\n",
       "              param_grid={'gradientboostingclassifier__learning_rate': [0.1,\n",
       "                                                                        0.2],\n",
       "                          'gradientboostingclassifier__n_estimators': [100, 200,\n",
       "                                                                       300]})}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the models by looping across pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "trained_models = {}\n",
    "for model_name, pipeline in mldev_pipeline.items():\n",
    "    #Train using CV class\n",
    "    print(f'Training model: {model_name}')\n",
    "    model = GridSearchCV(pipeline, hpgrid[model_name], n_jobs = -1, cv = 10) #n_jobs = -1 will use all processors for parallel computing\n",
    "    model.fit(Xohe_train_array, y_train.values.ravel())\n",
    "    trained_models[model_name] = model\n",
    "\n",
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for logreg: Accuracy= 0.8088026502602934 Precision= 0.6255060728744939 Recall= 0.5852272727272727 AUC= 0.7342540149125323 Confusion matrix= [[1400  185]\n",
      " [ 219  309]]\n",
      "\n",
      "Metrics for rf: Accuracy= 0.8135352579271179 Precision= 0.6419491525423728 Recall= 0.5738636363636364 AUC= 0.7336195153427015 Confusion matrix= [[1416  169]\n",
      " [ 225  303]]\n",
      "\n",
      "Metrics for gb: Accuracy= 0.8017037387600567 Precision= 0.6182212581344902 Recall= 0.5397727272727273 AUC= 0.7143658589045024 Confusion matrix= [[1409  176]\n",
      " [ 243  285]]\n"
     ]
    }
   ],
   "source": [
    "# Check metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    \n",
    "    y_hat = model.predict_proba(Xohe_test_array)[:,1]\n",
    "    #Converting to 1/0 based on threshold of 0.5\n",
    "    y_hat = (y_hat >=0.5).astype(int)\n",
    "    print(f'\\nMetrics for {model_name}:',\n",
    "    'Accuracy=', accuracy_score(y_test, y_hat),\n",
    "    'Precision=', precision_score(y_test,y_hat),\n",
    "    'Recall=', recall_score(y_test,y_hat),\n",
    "    'AUC=', roc_auc_score(y_test,y_hat),\n",
    "    'Confusion matrix=', confusion_matrix(y_test,y_hat)   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 10,\n",
       " 'error_score': nan,\n",
       " 'estimator__memory': None,\n",
       " 'estimator__steps': [('standardscaler', StandardScaler()),\n",
       "  ('logisticregression', LogisticRegression())],\n",
       " 'estimator__verbose': False,\n",
       " 'estimator__standardscaler': StandardScaler(),\n",
       " 'estimator__logisticregression': LogisticRegression(),\n",
       " 'estimator__standardscaler__copy': True,\n",
       " 'estimator__standardscaler__with_mean': True,\n",
       " 'estimator__standardscaler__with_std': True,\n",
       " 'estimator__logisticregression__C': 1.0,\n",
       " 'estimator__logisticregression__class_weight': None,\n",
       " 'estimator__logisticregression__dual': False,\n",
       " 'estimator__logisticregression__fit_intercept': True,\n",
       " 'estimator__logisticregression__intercept_scaling': 1,\n",
       " 'estimator__logisticregression__l1_ratio': None,\n",
       " 'estimator__logisticregression__max_iter': 100,\n",
       " 'estimator__logisticregression__multi_class': 'auto',\n",
       " 'estimator__logisticregression__n_jobs': None,\n",
       " 'estimator__logisticregression__penalty': 'l2',\n",
       " 'estimator__logisticregression__random_state': None,\n",
       " 'estimator__logisticregression__solver': 'lbfgs',\n",
       " 'estimator__logisticregression__tol': 0.0001,\n",
       " 'estimator__logisticregression__verbose': 0,\n",
       " 'estimator__logisticregression__warm_start': False,\n",
       " 'estimator': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression())]),\n",
       " 'n_jobs': -1,\n",
       " 'param_grid': {},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': None,\n",
       " 'verbose': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking parameters for the best model\n",
    "best_model = trained_models['logreg']\n",
    "display(best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking parameters for the best estimator of that model\n",
    "best_estimator = best_model.best_estimator_ #This will give a pipeline. To select an estimator you need to specify it as dictionary value.\n",
    "best_estimator['logisticregression'].get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model coefficients associated with best estimator of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28482825, -0.02430238, -0.31059975,  0.01193127, -0.01193127,\n",
       "         0.04400299, -0.08688219,  0.02951801, -0.00391402,  0.00391402,\n",
       "        -0.25924439,  0.31963964, -0.08688219, -0.59820256, -0.0934989 ,\n",
       "        -0.00482116,  0.09742271,  0.05445505, -0.08688219,  0.01858897,\n",
       "         0.10612894, -0.08688219, -0.03817172, -0.07384949,  0.07384949,\n",
       "        -0.00749402,  0.00749402, -0.03095295, -0.04558837,  0.11261968,\n",
       "        -0.05135655, -0.00482116,  0.00482116, -0.04919821,  0.04919821,\n",
       "        -0.07059261, -0.08688219,  0.1439082 , -0.0510463 , -0.08688219,\n",
       "         0.12509873,  0.10419691, -0.08688219, -0.03593396, -1.5140103 ,\n",
       "         0.77573186]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-1.65760655])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(best_estimator['logisticregression'].coef_)\n",
    "display(best_estimator['logisticregression'].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16008355049162734\n"
     ]
    }
   ],
   "source": [
    "# Lets check probability induced by bias\n",
    "import math\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "print(sigmoid(best_estimator['logisticregression'].intercept_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equates to 16%. This means that the model is 16% biased towards predictions without even looking at customer. This should be as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best model\n",
    "import pickle\n",
    "with open('bestmodel.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_models['logreg'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;logisticregression&#x27;,\n",
       "                                        LogisticRegression())]),\n",
       "             n_jobs=-1, param_grid={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;logisticregression&#x27;,\n",
       "                                        LogisticRegression())]),\n",
       "             n_jobs=-1, param_grid={})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression())]),\n",
       "             n_jobs=-1, param_grid={})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If we need to reload the model in future, we can use this code\n",
    "with open('bestmodel.pkl', 'rb') as f:\n",
    "    reloaded_model = pickle.load(f)\n",
    "\n",
    "reloaded_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know what model suits best for the data based on gridsearchCV method, we can train it from scratch later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xohe_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79107505, 0.79716024, 0.80324544, 0.77890467, 0.81541582,\n",
       "       0.82352941, 0.80324544, 0.80933063, 0.77687627, 0.81744422])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#With scaling\n",
    "logreg.fit(scaler.transform(Xohe_train_array), y_train.values.ravel())\n",
    "scores = cross_val_score(logreg, scaler.transform(Xohe_train_array), y_train.values.ravel(), cv=10)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8088026502602934 \n",
      "Precision= 0.6255060728744939 \n",
      "Recall= 0.5852272727272727 \n",
      "AUC= 0.7342540149125323 \n",
      "Confusion matrix= [[1400  185]\n",
      " [ 219  309]]\n"
     ]
    }
   ],
   "source": [
    "# Check metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "y_hat = logreg.predict_proba(scaler.transform(Xohe_test_array))[:,1]\n",
    "#Converting to 1/0 based on threshold of 0.5\n",
    "y_hat = (y_hat >=0.5).astype(int)\n",
    "\n",
    "print(\n",
    "'Accuracy=', accuracy_score(y_test, y_hat),\n",
    "'\\nPrecision=', precision_score(y_test,y_hat),\n",
    "'\\nRecall=', recall_score(y_test,y_hat),\n",
    "'\\nAUC=', roc_auc_score(y_test,y_hat),\n",
    "'\\nConfusion matrix=', confusion_matrix(y_test,y_hat)   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 'male',\n",
       " 'seniorcitizen': 'no',\n",
       " 'partner': 'yes',\n",
       " 'dependents': 'no',\n",
       " 'phoneservice': 'yes',\n",
       " 'multiplelines': 'yes',\n",
       " 'internetservice': 'fiber_optic',\n",
       " 'onlinesecurity': 'no',\n",
       " 'onlinebackup': 'yes',\n",
       " 'deviceprotection': 'no',\n",
       " 'techsupport': 'no',\n",
       " 'streamingtv': 'yes',\n",
       " 'streamingmovies': 'no',\n",
       " 'contract': 'one_year',\n",
       " 'paperlessbilling': 'yes',\n",
       " 'paymentmethod': 'bank_transfer_(automatic)',\n",
       " 'tenure': 67,\n",
       " 'monthlycharges': 88.4,\n",
       " 'totalcharges': 5798.3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted churn prob: [0.13085403]\n",
      "Actual churn: [0]\n"
     ]
    }
   ],
   "source": [
    "#Testing random sample prediction from test set\n",
    "sample_no = 5\n",
    "\n",
    "display(X_test_dicts[sample_no])\n",
    "print('Predicted churn prob:', logreg.predict_proba(scaler.transform(dv.transform(X_test_dicts[sample_no])))[:,1])\n",
    "print('Actual churn:', y_test.iloc[sample_no].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "import pickle\n",
    "#with open('model_fromscratch.pkl', 'wb') as f:\n",
    "#    pickle.dump(rfreg, f)\n",
    "\n",
    "pickle.dump(logreg, open('model_fromscratch.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving standardization params\n",
    "pickle.dump(scaler, open('scaling.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted churn prob: [0.34261714]\n",
      "Actual churn: [0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting parity plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(y_test, y_hat)\n",
    "plt.plot(y_test,y_test,'k-') \n",
    "plt.xlabel('True Life Expectancy')\n",
    "plt.ylabel('Predicted Life Expectancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting residuals\n",
    "import seaborn as sns\n",
    "\n",
    "residuals = y_test.values.ravel() - y_hat\n",
    "sns.displot(residuals, kind = 'kde')\n",
    "plt.xlabel('Residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatter plot for residuals and predictions\n",
    "plt.scatter(y_hat, residuals)\n",
    "plt.xlabel('Predicted Life Expectancy')\n",
    "plt.ylabel('Residuals')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c52f623ceac37ddf7faf652ecec1076e97b4693849be5fb4832d0833d01e4686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
